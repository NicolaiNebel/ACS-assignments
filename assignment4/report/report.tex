\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[head=26pt, a4paper, margin=1.2in, top=1.4in, bottom=1.75in]{geometry}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage[hidelinks, colorlinks, urlcolor=blue, linkcolor=black,citecolor=magenta]
{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage[table,xcdraw]{xcolor}

\usepackage{indentfirst}
\usepackage{a4wide}
\usepackage{color}
\usepackage{lipsum}
\usepackage{multicol}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes,positioning}

% ---------------- Page and margin/header/footer Setup -----------------
\pagestyle{fancy}
%\fancyhf{} % Clears header and footer
\fancyhead{}
\fancyfoot{}
\lhead{ACS --- Assignment 4}
\rhead{DIKU}
\lfoot{Page \thepage\ of \pageref{LastPage}}
\rfoot{Nicolai Jørgensen \\ Yiran Zhang}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
% ----------------------------------------------------------------------

\newtheorem{mythm}{Theorem}
\newtheorem{mydef}{Definition}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\title          {Assignment 4}
\author         {Nicolai Jørgensen and Yiran Zhang}

\begin{document}

\maketitle
\newpage

\section{Reliability}

We assume that $p$ is the probability of failure over some amount of time and we
are computing the probability of the system being connected after one such
amount of time.

\begin{enumerate}
  \item
    The system will become disconnected if one of the two wires fail. That means
    the failure probability is $Pr(X \leq 0)$, where $X \sim binom(2,p)$. That is,
    $X$ is a binomial-distributed random variable with $n = 2$ and $p = p$.
  \item
    In this case, the system will become disconnected if two of the three links
    fail, so the failure probability is $Pr(Y \leq 1)$ where $Y \sim binom(3,p)$.
  \item
    We compute the two probabilities:
    $$ Pr(X \leq 0) = { 2 \choose 0 } 0.000001^0(1-0.000001)^2 \approx 0.99999800 $$
    $$ Pr(X \leq 1) = \sum_{i=0}^1 { 3 \choose i }0.0001^i(1-0.0001)^{(3-i)} \approx 0.99999997 $$
    From this we conclude that the town council should buy the low-reliability links.
\end{enumerate}

\section{ARIES}

\begin{enumerate}
  \item
    Here is the dirty page table computed in the analysis phase:

    \begin{center}
      \begin{tabular}{ll}
      \rowcolor[HTML]{C0C0C0} 
      PageID & RecLSN \\
      P2     & 3      \\
      P1     & 4      \\
      P5     & 5      \\
      P3     & 6     
      \end{tabular}
    \end{center}

    And here is the transaction table:

    \begin{center}
      \centering
      \begin{tabular}{lll}
      \rowcolor[HTML]{C0C0C0} 
      TransID                    & Status & LastLSN \\
      \cellcolor[HTML]{6434FC}T1 & Active & 4       \\
      \cellcolor[HTML]{67FD9A}T2 & Active & 9      
      \end{tabular}
    \end{center}
  \item
    The set of winner transactions is $\{T3\}$ since it is the only one that finished.

    The set of loser transactions is $\{T1,T2\}$ since these did not finish before the crash.
  \item
    The redo phase starts at the minimum \verb|recLSN| in the dirty page table. This means 
    \verb|LSN 3|.

    The undo phase ends at the oldest \verb|LSN| of the transactions in the
    loser set. That would mean \verb|LSN 3|, since that is the first \verb|LSN|
    associated with \verb|T1|.
  \item
    The set of log records that may ause pages to be rewritten during the redo phase will
    consist of all \verb|update| or \verb|CLR| records after \verb|LSN 3|, where the redo phase starts.

    This means that the set is $\{3,4,5,6,8,9\}$.
  \item
    The set of log records to undo is the set of updates of the loser transactions. That means LSNs $\{9,8,5,4,3\}$.
  \item
    This is what is appended to the log after the recovery procedure is completed
    following a crash after \verb|LSN 10|.
    \begin{verbatim}
      LSN   LAST_LSN   TRAN_ID   TYPE             undoNextLSN  PAGE_ID
      ---   --------   -------   ----             -----------  -------
      11    9          T2        ABORT                         -
      12    4          T1        ABORT                         -
      13    11         T2        CLR: Undo LSN 9  8
      14    13         T2        CLR: Undo LSN 8  5
      15    14         T2        CLR: Undo LSN 5  -
      16    15         T2        end
      17    12         T1        CLR: Undo LSN 4  3
      18    17         T1        CLR: Undo LSN 3  -
      19    18         T1        end
    \end{verbatim}
\end{enumerate}

\section{Implementation}
\begin{enumerate}
  \item
    In our implementation, replicate requests from the master are handled
    synchronously, using the threadpool of the
    \textbf{CertainBookStoreReplicator} object. Each of the replication requests
    are handled by a \textbf{Callable} \textbf{CertainBookStoreReplicationTask}
    object, that simply calls the HTTP proxys replicate function. The proxy
    uses \textbf{performHTTPRequest} to ask the clients to replicate using a
    \textbf{REPLICATE} signal. Here we assume that outside actors can't send
    requests directly to our backend, since that would allow anyone to
    fabricate replicate requests.

    In \textbf{SlaveCertainBookStore}, the replication requests are multiplexed
    and the corresponding methods are called.  Here, we make use of the
    fail-stop assumption: If anything goes wrong in the bookstore, it is going
    to fail immediately with no period of invalid state. Then, the system is
    going to recognize that the component has failed and can be restarted.\\

    We also implemented load balancing for incoming requests in
    \textbf{ReplicationAwareBookStoreHTTPProxy} and
    \textbf{ReplicationAwareStockManagerHTTPProxy}. We did so using a
    randomized method. Here we assume some amount of profiling work has been
    done, because we define a constant \textbf{EXP\_PERCENT\_WRITES}, which
    define the expected number of write requests compared to the total number
    of write requests to the service.  Then, we compute 

    $$p_{master} = \frac{1}{\#slaves + 1} - EXP\_PERCENT\_WRITES$$
    
    This defines the share of read requests that the master unit should receive
    such that it has the same expected number of requests as the slaves.  Then,
    if $p_{master}$ is positive we pick the master with $p_{master}$. If we
    don't pick it, or if $p_{master}$ is negative, we pick a slave unit
    uniformly at random.

    This way, every unit has the same number of expected requests.\\

    For our testing we added two tests: \textbf{testWriteFailureCanStillRead}
    where we cause the master to throw an exception and then ensure that
    the service still processes read requests, and \textbf{testSlaveErrorMasking},
    where we test that a failure in a slave causes no change in availability.

  \item
    The obvious advantage is that the read requests can be distriuted over an
    amount of servers, which should provide both a reliability and performance
    boost. Another advantage is that the scheme is simple. We know that write
    requests always are performed on the single master and then sent to each of
    the slaves. This means that synchronizing servers is a simple process.
    Compare this to the case where multiple servers perform writes
    synchronously. Then we would need a much more advanced replication scheme
    to ensure consistency.

    With regard to performance, we expect that performance should increase in a
    read-heavy workload, since this makes good use of parallellism. Write requests
    will still slow the system down and constitutes a bottleneck. Every write request
    still has to be handled on every server and then there is the added
    overhead of sending HTTP requests back and forth on the backend.

  \item
    The client should simply state its latest timestamp seen. Then the proxy
    can make sure that a response is only sent to the client with a later timestamp.
    If this is not yet possible, it can delay the request.

    Here we make use of the assumption that timestamps consistently refer to
    the same states across all servers.

  \item
    If a network partition seperated some slaves from the master, they would no longer
    be able to receive replicate requests. Then, the master would mark these servers as
    failed slaves and continue replicating to the slaves it can still see. The seperated
    slaves would then never update their state again.

    A client seeing this old state might conceivably at some point send illegal requests
    to the master, causing it to fail as well.

\end{enumerate}

\end{document}
