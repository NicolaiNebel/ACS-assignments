\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[head=26pt, a4paper, margin=1.2in, top=1.4in, bottom=1.75in]{geometry}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage[hidelinks, colorlinks, urlcolor=blue, linkcolor=black,citecolor=magenta]
{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\usepackage{mathtools}

% ---------------- Page and margin/header/footer Setup -----------------
\pagestyle{fancy}
%\fancyhf{} % Clears header and footer
\fancyhead{}
\fancyfoot{}
\lhead{ACS --- Assignment 1}
\rhead{DIKU}
\lfoot{Page \thepage\ of \pageref{LastPage}}
\rfoot{Yiran Zhang\\Nicolai Jørgensen}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
% ----------------------------------------------------------------------

\newtheorem{mythm}{Theorem}
\newtheorem{mydef}{Definition}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\title          {Assignment 1}
\author         {Yiran Zhang and Nicolai Jørgensen}

\begin{document}

\maketitle
\newpage

\section{Question 1}

\begin{enumerate}

  \item
    In order to organise memory with physical storage on several machines, we are
    going to split the top-level memory into pages. Additionally, we will maintain a
    map between the top-level pages and a tuple of machine identifier and local
    address. The mapping can be implemented to run in $O(log n)$ time, with $n$ being the
    number of allocated pages, which can be kept small by choosing suitably large
    page sizes.

    In addition, we will for each machine maintain a list of its pages and the
    number of pages it has allocated. The length of the list can be used to load
    balance the system, by evenly splitting the memory between available machines.
    When a machine leaves the system, all of its pages can be copied to prevent data
    loss.

  \item
    In the pseudocode below, we assume calls to address directly into the big
    contiguous memory space. That is, there is no virtual memory addressing going
    on.
\begin{Verbatim}
READ(addr):
  (PageNo, Offset) = (addr / page_size, addr % page_size)
  if (exists(page_map, PageNo)):
    (Machine, MachineAddr) = lookup(page_map, PageNo)
    v = RemoteREAD(Machine, MachineAddr + Offset)
    if      v == segfault: return segfault
    else if v == timeout:  try again a set number of times, if still no success, return timeout
    else:                  return v
  else:
    return segfault

WRITE(addr, value):
  (PageNo, Offset) <- (addr / page_size, addr % page_size)
  if (exists(page_map, PageNo)):
    (Machine, MachineAddr) = lookup(page_map, PageNo)
    return RemoteWRITE(Machine, MachineAddr + Offset, value)
  else:
    if system has space for a new page:
      (Machine, MachineAddr) = allocate_new_page(PageNo),
      add(page_map, PageNo, (Machine, MachineAddr))
      RemoteWRITE(Machine, MachineAddr + Offset, value)
    else:
      return segfault

RemoteREAD(Machine, Addr):
  SEND(Machine, \{ READ, Addr \})
  RECEIVE(Machine, Value)
  On timeout:  return timeout
  else:        return Value

RemoteWRITE(Machine, Addr, Value):
  SEND(Machine, \{ WRITE, Addr, Value \})

allocate_new_page(PageNo):
  find machine with least pages (O(logn))
  try to allocate page:
  on fail:
    remove machine from list of available machines for allocation
    allocate_new_page(PageNo)
  on success:
    return (machine, allocated page addr)

\end{Verbatim}
    The page numbers and offsets are calculated using simple integer division and
    modulo. The functions \verb|lookup|, \verb|get| and \verb|add| refer to a map
    structure with $O(logn)$ running times implemented with e.g a binary search
    tree.

  \item
    We believe that memory access against the unified memory space need not
    necessarily be atomic. However, memory access to addresses in individual
    machines still need to preserve this basic integrity. That is, we should be able
    to distribute the memory access computation between machines.

  \item
    Our name mapping strategy makes an assumption about the system setup.
    We assume that we know the addresses, and thus also the quantity, of machines in
    the unified memory space. Then, we use that information to dynamically spread
    allocated pages over the machines.

    Our system also allows for dynamic leaves and joins of machines in the memory
    space. Joining is simple, we just inform the system that a new machine is
    available with no pages allocated yet. Leaving is a bit more complicated, but
    can be done by iterating over the pages it had allocated. Each page should be
    allocated on and copied unto another machine. The leave operation should inform
    the system if some data could not be copied.

\section{Question 2}

\begin{enumerate}
  \item
    Concurrency may influence latency positively or negatively, depending on where
    and how it is applied. If a task can be split into multiple independent parts,
    computing each of the parts concurrently can reduce the overall processing time and
    thus the latency of the system. Similarly, if a system receives independent
    requests from a number of clients, then having extra processing units will
    decrease the average latency of a request.  Concurrency may not always result in
    a latency improvement. Parallelizing a program is not free. There is an amount
    of overhead that is incurred when the program has to coordinate its subordinate
    threads. Similarly, some threads may stall for periods of time waiting for
    intermediate results not yet computed.

    Concurrency may provide a latency boost, but there are individual considerations
    to make none the less. These are complexity of programming, applicability of
    concurrency to the problem and overhead incurred.

  \item
    Batching is the process of bundling several transactions or messages into a
    single one in order to reduce the overhead. Batching naturally arises in
    program bottlenecks, where the requests will tend to pile up. An example is
    memory access to the hard disc. Memory access is really slow, and requests
    might pile up while another access is being processed. Batching similar
    requests together will reduce the overhead of sending individual requests
    back and forth.

    Dallying is a strategy for handling requests, which consists of speculatively
    delaying the processing of a request. If many requests accumulate through
    dallying they can be batched together, or the result of the request might not be
    needed after all. In the case of memory access from before, non-critical
    requests may be delayed until a batch process can take care of many at once.
    Another example of dallying is lazy evaluation in Haskell. Computations are
    delayed until such a point that their values are actually needed. This
    allows programmers to make outrageous requests such as infinitely recursive
    data structures without looping forever.

  \item
    Here I will assume that by caching is meant the memory system optimization
    that utilizes fast memory hardware to mask the latency of hard disc storage.

    Caching is indeed an example of a fast path optimization. Fast path optimization
    refers to designing system to make it fast in the common case. The concept of
    "locality of reference" occurs almost naturally when designing programs.
    We are specifically referring to temporal and spatial locality, which means that
    if one memory address is accessed, it and its neighbours are more likely to be
    accessed in the near future. Caching the entire page of an address makes lookups
    to these addresses faster, thus optimizing the common case.

    If by caching was meant web browsers storing recently visited web pages,
    then it is indeed a fast path optimization in almost the same way. Users
    will often want to go back in their history to look at a recent page. Having
    it already loaded then avoids having to do an expensive and redundant
    network exchange with the web server.
\end{enumerate}



\end{document}
