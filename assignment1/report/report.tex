\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[head=26pt, a4paper, margin=1.2in, top=1.4in, bottom=1.75in]{geometry}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage[hidelinks, colorlinks, urlcolor=blue, linkcolor=black,citecolor=magenta]
{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{enumitem}

% ---------------- Page and margin/header/footer Setup -----------------
\pagestyle{fancy}
%\fancyhf{} % Clears header and footer
\fancyhead{}
\fancyfoot{}
\lhead{ACS --- Assignment 1}
\rhead{DIKU}
\lfoot{Page \thepage\ of \pageref{LastPage}}
\rfoot{Yiran Zhang\\Nicolai Jørgensen}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
% ----------------------------------------------------------------------

\newtheorem{mythm}{Theorem}
\newtheorem{mydef}{Definition}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\title          {Assignment 1}
\author         {Yiran Zhang and Nicolai Jørgensen}

\begin{document}

\maketitle
\newpage

\section{Question 1}

\begin{enumerate}

  \item
    In order to organise memory with physical storage on several machines, we are
    going to split the top-level memory into pages. Additionally, we will maintain a
    map between the top-level pages and a tuple of machine identifier and local
    address. The mapping can be implemented to run in $O(log n)$ time, with $n$ being the
    number of allocated pages, which can be kept small by choosing suitably large
    page sizes.

    In addition, we will for each machine maintain a list of its pages and the
    number of pages it has allocated. The length of the list can be used to load
    balance the system, by evenly splitting the memory between available machines.
    When a machine leaves the system, all of its pages can be copied to prevent data
    loss.

  \item
    In the pseudocode below, we assume calls to address directly into the big
    contiguous memory space. That is, there is no virtual memory addressing going
    on.
\begin{verbatim}
READ(addr):
  (PageNo, Offset) = (addr / page_size, addr % page_size)
  if (exists(page_map, PageNo)):
    (Machine, MachineAddr) = lookup(page_map, PageNo)
    v = RemoteREAD(Machine, MachineAddr + Offset)
    if      v == segfault: return segfault
    else if v == timeout:  try again a set number of times, if still no success, return timeout
    else:                  return v
  else:
    return segfault

WRITE(addr, value):
  (PageNo, Offset) <- (addr / page_size, addr % page_size)
  if (exists(page_map, PageNo)):
    (Machine, MachineAddr) = lookup(page_map, PageNo)
    return RemoteWRITE(Machine, MachineAddr + Offset, value)
  else:
    if system has space for a new page:
      (Machine, MachineAddr) = allocate_new_page(PageNo),
      add(page_map, PageNo, (Machine, MachineAddr))
      RemoteWRITE(Machine, MachineAddr + Offset, value)
    else:
      return segfault

RemoteREAD(Machine, Addr):
  SEND(Machine, \{ READ, Addr \})
  RECEIVE(Machine, Value)
  On timeout:  return timeout
  else:        return Value

RemoteWRITE(Machine, Addr, Value):
  SEND(Machine, \{ WRITE, Addr, Value \})

allocate_new_page(PageNo):
  find machine with least pages (O(logn))
  try to allocate page:
  on fail:
    remove machine from list of available machines for allocation
    allocate_new_page(PageNo)
  on success:
    return (machine, allocated page addr)

\end{verbatim}
    The page numbers and offsets are calculated using simple integer division and
    modulo. The functions \verb|lookup|, \verb|get| and \verb|add| refer to a map
    structure with $O(logn)$ running times implemented with e.g a binary search
    tree.

  \item
    We believe that memory access against the unified memory space need not
    necessarily be atomic. However, memory access to addresses in individual
    machines still need to preserve this basic integrity. That is, we should be able
    to distribute the memory access computation between machines.

  \item
    Our name mapping strategy makes an assumption about the system setup.
    We assume that we know the addresses, and thus also the quantity, of machines in
    the unified memory space. Then, we use that information to dynamically spread
    allocated pages over the machines.

    Our system also allows for dynamic leaves and joins of machines in the memory
    space. Joining is simple, we just inform the system that a new machine is
    available with no pages allocated yet. Leaving is a bit more complicated, but
    can be done by iterating over the pages it had allocated. Each page should be
    allocated on and copied unto another machine. The leave operation should inform
    the system if some data could not be copied.
\end{enumerate}

\section{Question 2}

\begin{enumerate}
  \item
    Concurrency may influence latency positively or negatively, depending on where
    and how it is applied. If a task can be split into multiple independent parts,
    computing each of the parts concurrently can reduce the overall processing time and
    thus the latency of the system. Similarly, if a system receives independent
    requests from a number of clients, then having extra processing units will
    decrease the average latency of a request.  Concurrency may not always result in
    a latency improvement. Parallelizing a program is not free. There is an amount
    of overhead that is incurred when the program has to coordinate its subordinate
    threads. Similarly, some threads may stall for periods of time waiting for
    intermediate results not yet computed.

    Concurrency may provide a latency boost, but there are individual considerations
    to make none the less. These are complexity of programming, applicability of
    concurrency to the problem and overhead incurred.

  \item
    Batching is the process of bundling several transactions or messages into a
    single one in order to reduce the overhead. Batching naturally arises in
    program bottlenecks, where the requests will tend to pile up. An example is
    memory access to the hard disc. Memory access is really slow, and requests
    might pile up while another access is being processed. Batching similar
    requests together will reduce the overhead of sending individual requests
    back and forth.

    Dallying is a strategy for handling requests, which consists of speculatively
    delaying the processing of a request. If many requests accumulate through
    dallying they can be batched together, or the result of the request might not be
    needed after all. In the case of memory access from before, non-critical
    requests may be delayed until a batch process can take care of many at once.
    Another example of dallying is lazy evaluation in Haskell. Computations are
    delayed until such a point that their values are actually needed. This
    allows programmers to make outrageous requests such as infinitely recursive
    data structures without looping forever.

  \item
    Here I will assume that by caching is meant the memory system optimization
    that utilizes fast memory hardware to mask the latency of hard disc storage.

    Caching is indeed an example of a fast path optimization. Fast path optimization
    refers to designing system to make it fast in the common case. The concept of
    "locality of reference" occurs almost naturally when designing programs.
    We are specifically referring to temporal and spatial locality, which means that
    if one memory address is accessed, it and its neighbours are more likely to be
    accessed in the near future. Caching the entire page of an address makes lookups
    to these addresses faster, thus optimizing the common case.

    If by caching was meant web browsers storing recently visited web pages,
    then it is indeed a fast path optimization in almost the same way. Users
    will often want to go back in their history to look at a recent page. Having
    it already loaded then avoids having to do an expensive and redundant
    network exchange with the web server.
\end{enumerate}


Caching is indeed an example of a fast path optimization. Fast path optimization
refers to designing system to make it fast in the common case. The concept of
"locality of reference" occurs almost naturally when designing programs.
We are specifically referring to temporal and spatial locality, which means that
if one memory address is accessed, it and its neighbours are more likely to be
accessed in the near future. Caching the entire page of an address makes lookups
to these addresses faster, thus optimizing the common case.

\section{Questions for Discussion on Architecture}

\begin{enumerate}
  \item
    \begin{itemize}
      \item[a)] when the implementation and tests succeeds it will return a result otherwise it will throw an exception
      \item[b)]
        For the \texttt{rateBooks} we write the following tests: test that a
        single valid rating is processed correctly, test that multiple ratings
        accumulate on a book, test test books cannot be rated if ISBN is
        invalid, test that books cannot be rated if a rating is invalid,test
        that trying to rate a book not in the score causes an error.

        For the \texttt{getTopRatedBooks} we write the following tests: test
        that books can not be got if K is not valid, tests that a valid K can
        be processed correctly.

        For the \texttt{getBooksIndemand} we tests that books in demand can be
        retrived.
    \end{itemize}
  \item
    \begin{itemize}
      \item[a)]
        In this architecture the customer sole is strongly modular. The
        \texttt{StockManager} and \texttt{BookStore} are complete separation.And they
        both have separate \texttt{HTTPProxies}. 

        And all the functions corresponding to the two customer role can be called
        separately.

      \item[b)]
        The isolation is fact that the bookstore implementation and our clients have to
        exchange communication through a distinct layer, and we use RPCs to ask a
        server for the actual computation stuff

        The protection is fact that the architecture has strong modularity which means
        it has low coupling. When one modular fails, the system can run as well

      \item[c)]
        The same kind of isolation is not enforced. Once the JVM breaks down,both the
        server and the clients will crash, as possible through the test
    \end{itemize}
  \item
    \begin{itemize}
      \item[a)]
        Yes, there is naming system, it binds the names with the address or
        resource.  By specifying the name  we can get the resource, the service
        address and the provider information. Therefore users can interact with a
        computer through names.

      \item[b)]
        We use IP address to allows the clients to discover and communicate with
        servers
    \end{itemize}
  \item
    At-most-once semantics is implemented in the architecture. Using at-most-once
    the RPCs will either return a calling result or throw an exception. In out
    implementation, when the RPC succeeds it will return the result, otherwise it
  \item
    \begin{itemize}
      \item[a)]
        Yes it is safe to use web proxy servers with the architecture of Figure 2
      \item[b)]
        Because it encrypt the communication to and from the clients. And the proxies
        should be deployed between the software proxies and the server where it uses
        the HTTP requests and responses throws an exception and fails.
    \end{itemize}
  \item
    \begin{itemize}
      \item[a)]
        Yes there is scalability bottlenecks in this architecture with respect to the
        number of clients
      \item[b)]
        CPU, the RAM and the hard disk.
    \end{itemize}
  \item
    \begin{itemize}
      \item[a)]
        No.\ when no cache is employed in the web proxy,the clients experience failures
        similar if web proxies were used in the architecture.

        Because the web proxies cannot store the data in cache, when the sever crash
        the web proxies cannot fetch the data from the remote sever and cannot return
        corresponding reply. Therefore the clients can experience the similar failure
      \item[b)]
        Yes, because web proxies store the web pages and some files in cache so when
        the sever crash, the web proxies can return the some datas in the cache to the
        clients
      \item[c)]
        When using at-most-once semantics, if it fails, the RPCs returns results or
        throws exception and the web proxies stores the results or exceptions.
        Therefore in the next the time, if the clients send the same request, the web
        proxies will directly return the results or exceptions.
    \end{itemize}
\end{enumerate}

\end{document}
